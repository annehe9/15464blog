### April 21th, 2021 | Lecture 21

The first talk had some nice results. The deformation of the face looked realistic and the expressions matched what was happening in the conversation. I agree the lack of movement in the eyes looked uncanny. I am still a bit unconvinced to how it works. As with a lot of machine learning based systems it seems like a black box.
The Neural Voice Puppetry paper was a little unsettling in the potential of creating deepfakes. The JALI paper seems like a great tool for animators. It was cool to see it used in Cyberpunk, though I was initially skeptical on how well it was executed, knowing that Cyberpunk was infamous for having a ton of bugs on release. The final result in game still did not stand out to me as being particularly impressive, but it probably saved the animators a lot of time. It was nice to see the subtle changes in facial expression based on language. 
