### February 24th, 2021 | Lecture 6
It was nice to hear people present about some new papers today.

Vashnavi presented a paper about tracking hand motion, with a focus on self contact and occulusion created by the creases and bulges of the hand. This is quite interesting because now that I think about it, most major hand motion research does not seem to take this into account. Hands are really very fleshy and there are some interesting properties and characteristics that can be observed from the results of this paper. I agree that the speed of this method could be further optimized, but they do have to keep track of many physics properties at every frame and if a frame is miscalculated, this could cause drift, so it will definitely be difficult to optimize for speed while maintaining the realistic properties.

Then, Anna presented a paper on 3D stop motion facial animation. I enjoyed the animation in Kubo and the Two Strings so it was very interesting to see how they produce modern stop motion animation. It seems like it would definitely be tedious and expensive to create so many 3D models. I wonder why they decided to split the face into the top and bottom as opposed to some other division. It was impressive how many frames they were able to capture with just 2 libraries and applying deformations between the two parts of the face. I think it makes sense that the number of frames doesnt scale linearly, because the output is a combination of parts in the different libraries (so maybe a n^2 factor?). It also takes fewer libraries to increase the number of frames at a certain point probably because it takes many diverse parts to span basic expressions, and from there, it is easier to add more subtle changes. Overall, I thought the facial animation looked really good and believable, which was impressive because of how expressive facial animation needs to be and how easily we can perceive errors in facial motion.

Nikolas presented the classic Pinocchio paper for rigging and animating a mesh. It was nice to get a clear walkthrough of the process and it gave a good background on what things are built upon today.

Then, Nathan presented a paper on motion synthesis for social conversations. It was cool to see a presentation on a paper out of CMU. I think the indicators they chose to divide the speech into were good, though I do wish they had more correlation between the speech and the hand motions chosen. They did do a good job of matching the peaks of motion with the peaks in the audio. Overall, I noticed that many of the matched motions, while there was more motion during the emphasized parts or when people were speaking, and the motions themselves looked realistic, it didnt really match the emotion being expressed. I wonder if this could be combined with other research on audio and emotion to better categorize and associate character motion and emotion.

Lastly, Alan presented on skeleton aware motion retargeting. I thought it was interesting how they could convert the motion from a variety of complex skeletons to a primal skeleton, and then to a different complex skeleton. The results seemed quite good and they adapted well to characters with different joint and bone structure and uneven skeletons.

